{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1929264,"sourceType":"datasetVersion","datasetId":1150837}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sergeypolivin/language-recognition-using-bert?scriptVersionId=218144923\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    AutoModelForSequenceClassification, \n    AutoTokenizer,\n    logging, \n    pipeline,\n)\n\nDATA_DIR = \"/kaggle/input/language-detection/\"\nMODEL_NAME = \"bert-base-multilingual-uncased\"\nRANDOM_STATE = 12345\nBATCH_SIZE = 64\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nNUM_EPOCHS = 1\nLEARNING_RATE = 5e-5\n\ntorch.backends.cudnn.deterministic = True\ntorch.manual_seed(RANDOM_STATE)\nlogging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying the device used for fune-tuning BERT\nprint(f\"Device used: {DEVICE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"# Loading the data\nlang_data = pd.read_csv(DATA_DIR + \"Language Detection.csv\")\n\n# Displaying random 30 rows\nlang_data.sample(n=30, random_state=RANDOM_STATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying a concise summary of the data\nlang_data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data exploration","metadata":{}},{"cell_type":"code","source":"# Plotting the structure of the target variable\nlang_counts = lang_data[\"Language\"].value_counts(normalize=True)\nlang_counts.plot(kind=\"bar\")\nplt.title(\"Shares of objects in each language class\", fontsize=15)\nplt.ylabel(\"Proportion of objects\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encoding labels","metadata":{}},{"cell_type":"code","source":"# Retrieving the text data\ntexts_data = lang_data[\"Text\"].values.astype(\"U\")\n\n# Retrieving the labels data\nlabels_data = lang_data[\"Language\"].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiating the LabelEncoder object\nlabel_encoder = LabelEncoder()\n\n# Encoding the labels\nlabels_data_encoded = label_encoder.fit_transform(labels_data)\nclass_names = label_encoder.classes_\n\n# Displaying the encoding results\nfor idx, class_name in enumerate(class_names):\n    print(f\"{idx:<2} => {class_name}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting data","metadata":{}},{"cell_type":"code","source":"# Separating data into training set and validation/test sets\n(\ntraining_texts, \nvalidation_testing_texts, \ntraining_labels, \nvalidation_testing_labels\n) = train_test_split(\n    texts_data,\n    labels_data_encoded,\n    train_size=0.8,\n    random_state=RANDOM_STATE,\n    stratify=labels_data_encoded,\n)\n\n# Separating validation and test sets\n(\nvalidation_texts, \ntesting_texts, \nvalidation_labels, \ntesting_labels\n) = train_test_split(\n    validation_testing_texts,\n    validation_testing_labels,\n    train_size=0.5,\n    random_state=RANDOM_STATE,\n    stratify=validation_testing_labels,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verifying the correctness of dimensions\nassert (\n    training_texts.shape[0] + validation_texts.shape[0] + testing_texts.shape[0] \\\n    == texts_data.shape[0]\n)\n\n# Displaying the number of objects in each set\ntraining_texts.shape[0], validation_texts.shape[0], testing_texts.shape[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data vizualization","metadata":{}},{"cell_type":"code","source":"def plot_target_structure(labels, fig_title=\"Title_1\"):\n    \"\"\"Plotting the shares of Dataset labels.\"\"\"\n    # Computing the unique labels\n    unique_labels = np.unique(labels)\n    # Computing the number of objects within each class\n    labels_count = np.bincount(labels)\n    \n    # Computing the shares of objects in each class\n    n_obj = labels.shape[0]\n    \n    labels_info_share = pd.Series(\n        labels_count, index=label_encoder.classes_\n    ) / n_obj\n    \n    # Plotting a figure\n    labels_info_share.plot(kind=\"bar\")\n    plt.xticks(rotation=0)\n    plt.title(fig_title, fontsize=15)\n    plt.xlabel(\"Class name\")\n    plt.ylabel(\"Proportion of objects\")\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_target_structure(\n    labels=labels_data_encoded, fig_title=\"Full data set\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_target_structure(\n    labels=training_labels, fig_title=\"Training set\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_target_structure(\n    labels=validation_labels, fig_title=\"Validation set\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_target_structure(\n    labels=testing_labels, fig_title=\"Test set\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# Instantiating a BERT tokenizer\nbert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenizing the training examples\ntraining_encodings = bert_tokenizer(\n    list(training_texts),\n    add_special_tokens=True,\n    max_length=128,\n    truncation=True, \n    padding=\"max_length\",\n)\n\n# Tokenizing the validation examples\nvalidation_encodings = bert_tokenizer(\n    list(validation_texts),\n    add_special_tokens=True,\n    max_length=128,\n    truncation=True, \n    padding=\"max_length\",\n)\n\n# Tokenizing the testing examples\ntesting_encodings = bert_tokenizer(\n    list(testing_texts),\n    add_special_tokens=True,\n    max_length=128,\n    truncation=True,\n    padding=\"max_length\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_encodings_info(\n    tokenizer, \n    encodings, \n    texts, \n    labels,\n    text_idx\n):\n    \"\"\"Shows the original, encoded and decoded texts.\"\"\"\n    # Displaying the original text\n    text = texts[text_idx]\n    print(f\"Input text:\\n{text}\\n\")\n    \n    # Displaying the language of the text\n    lang_label = labels[text_idx]\n    lang = class_names[lang_label]\n    print(f\"Language: {lang}\\n\")\n    \n    # Displaying the encoded text\n    text_encoded = encodings[\"input_ids\"][text_idx]\n    print(f\"Tokenized input text (encoded):\\n{text_encoded}\\n\")\n    \n    # Displaying the decoded text\n    text_decoded = tokenizer.convert_ids_to_tokens(text_encoded)\n    print(f\"Tokenized input text (decoded):\\n{text_decoded}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_encodings_info(\n    tokenizer=bert_tokenizer,\n    encodings=training_encodings, \n    texts=training_texts, \n    labels=training_labels,\n    text_idx=990,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_encodings_info(\n    tokenizer=bert_tokenizer,\n    encodings=validation_encodings, \n    texts=validation_texts, \n    labels=validation_labels,\n    text_idx=1033,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_encodings_info(\n    tokenizer=bert_tokenizer,\n    encodings=testing_encodings, \n    texts=testing_texts, \n    labels=testing_labels,\n    text_idx=1010,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating Datasets and Dataloaders","metadata":{}},{"cell_type":"code","source":"class LanguageDataset(Dataset):\n    \"\"\"Class for creating a custom dataset.\"\"\"\n    \n    def __init__(self, encodings, labels):\n        \"\"\"Constructor for LanguageDataset class.\"\"\"\n        self.encodings = encodings\n        self.labels = labels\n        \n    def __len__(self):\n        \"\"\"Computes the number of the dataset objects.\"\"\"\n        dataset_length = len(self.labels)\n        \n        return dataset_length\n\n    def __getitem__(self, idx):\n        \"\"\"Returns the corresponding samples for index given.\"\"\"\n        item = {key: torch.tensor(value[idx])\n                for key, value in self.encodings.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        \n        return item","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initializing the training dataset\ntraining_dataset = LanguageDataset(\n    encodings=training_encodings, \n    labels=training_labels,\n)\n\n# Initializing the validation dataset\nvalidation_dataset = LanguageDataset(\n    encodings=validation_encodings, \n    labels=validation_labels,\n)\n\n# Initializing the testing dataset\ntesting_dataset = LanguageDataset(\n    encodings=testing_encodings, \n    labels=testing_labels,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a training Dataloader\ntraining_dataloader = DataLoader(\n    training_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n)\n\n# Creating a validation Dataloader\nvalidation_dataloader = DataLoader(\n    validation_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False,\n)\n\n# Creating a testing Dataloader\ntesting_dataloader = DataLoader(\n    testing_dataset,\n    batch_size=BATCH_SIZE, \n    shuffle=False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Training data examples: {len(training_dataloader.dataset):,}\")\nprint(f\"Number of batches: {len(training_dataloader)}\")\nprint(f\"Batch size: {BATCH_SIZE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Validation data examples: {len(validation_dataloader.dataset)}\")\nprint(f\"Number of batches: {len(validation_dataloader)}\")\nprint(f\"Batch size: {BATCH_SIZE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Testing data examples: {len(testing_dataloader.dataset):,}\")\nprint(f\"Number of batches: {len(testing_dataloader)}\")\nprint(f\"Batch size: {BATCH_SIZE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining the BERT model","metadata":{}},{"cell_type":"code","source":"# Creating a mapping from predictions to label names\nid2label_mappings = dict()\nfor i, name in enumerate(class_names):\n    id2label_mappings[i] = name\nid2label_mappings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Computing the number of classes\nnum_labels = len(class_names)\n\n# Instantiating the BERT model\nbert_model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME, \n    num_labels=num_labels, \n    id2label=id2label_mappings,\n)\n\n# Moving the model to DEVICE (GPU/CUDA)\nbert_model.to(DEVICE)\n\n# Defining the optimization algorithm\noptimizer = torch.optim.Adam(bert_model.parameters(), lr=LEARNING_RATE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_model.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Computing accuracy","metadata":{}},{"cell_type":"code","source":"def acc_score(model, dataloader, device=DEVICE):\n    \"\"\"Computes the accuracy score for a DataLoader.\"\"\"\n    # Preallocating counter variables\n    correct_predictions, num_examples = 0, 0\n    \n    # Turning off computing gradients\n    with torch.no_grad():\n        \n        # Iteratively computing accuracy score (batch by batch)\n        for batch_idx, batch in enumerate(dataloader):\n            \n            # Selecting the batch data (encodings, attention mask, labels)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            # Using BERT to compute logits\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs[\"logits\"]\n            \n            # Computing the predictions for labels\n            predicted_labels = torch.argmax(logits, dim=1)\n            \n            # Computing the number of examples/correct predictions number\n            num_examples += labels.size(0)\n            correct_predictions += (predicted_labels == labels).sum()\n    \n    # Computing the final accuracy score\n    accuracy_score = correct_predictions.float() / num_examples\n        \n    return accuracy_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-tuning loop","metadata":{}},{"cell_type":"code","source":"def train_bert_model(\n    model, \n    optimizer, \n    training_dataloader, \n    validation_dataloader,\n    accuracy_score_func=acc_score,\n    epochs=2,\n    batch_log_freq=100,\n    device=DEVICE\n):\n    \"\"\"Launches the fine-tuning of BERT.\"\"\"\n    # Starting the timer\n    start_time = time.time()\n    \n    # Going through all epochs\n    for epoch in range(epochs):\n        \n        # Setting the model in the training mode\n        model.train()\n        \n        # Going through all batches\n        for batch_idx, batch in enumerate(training_dataloader):\n        \n            # Selecting the batch\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # BERT forward pass\n            outputs = model(\n                input_ids, attention_mask=attention_mask, labels=labels\n            )\n            loss, logits = outputs[\"loss\"], outputs[\"logits\"]\n        \n            # BERT backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            # Logging the progress\n            if not batch_idx % batch_log_freq:\n                print (f\"Epoch {epoch+1:03d}/{epochs:03d} | \"\n                       f\"Batch {batch_idx:03d}/{len(training_dataloader):03d} | \"\n                       f\"Loss = {loss:.4f}\")\n        \n        # Setting the model in the evaluation mode\n        model.eval()\n        \n        # Disabling computing gradients\n        with torch.set_grad_enabled(False):\n            # Computing training accuracy\n            training_accuracy_score = accuracy_score_func(\n                model=model,\n                dataloader=training_dataloader,\n            )\n            # Computing validation accuracy\n            validation_accuracy_score = accuracy_score_func(\n                model=model,\n                dataloader=validation_dataloader,\n            )\n            # Logging the accuracy scores\n            print(f\"\\nTraining accuracy = \"\n                  f\"{training_accuracy_score:.4f}\"\n                  f\"\\nValid accuracy = \"\n                  f\"{validation_accuracy_score:.4f}\\n\")\n        \n        # Printing the time passed at the end of the epoch\n        time_elapsed_epoch = (time.time() - start_time) / 60\n        print(f'Time elapsed: {time_elapsed_epoch:.2f} min\\n')\n    \n    # Printing the total time spent on BERT fine-tuning\n    time_elapsed_total = (time.time() - start_time) / 60\n    print(f'\\nTotal training Time: {time_elapsed_total:.2f} min')\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training the BERT model\nbert_model = train_bert_model(\n    model=bert_model,\n    optimizer=optimizer,\n    training_dataloader=training_dataloader,\n    validation_dataloader=validation_dataloader,\n    epochs=NUM_EPOCHS,\n    batch_log_freq=10,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_test(model, dataloader, device=DEVICE):\n    \"\"\"Predicts the labels for the DataLoader.\"\"\"\n    # Setting up counter variables\n    correct_preds, num_examples = 0, 0\n    # Preallocating the list for test predictions\n    test_predictions = []\n    \n    # Disabling computing gradients\n    with torch.no_grad():\n        \n        # Iterating through all batches\n        for batch_idx, batch in enumerate(dataloader):\n            \n            # Selecting the batch\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            # Computing logits\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs[\"logits\"]\n            \n            # Computing the predictions for labels\n            predicted_labels_batch = torch.argmax(logits, dim=1)\n            \n            # Adding the batch predictions to the list\n            test_predictions.append(predicted_labels_batch)\n            \n            # Iteratively computing accuracy determinants\n            num_examples += labels.size(0)\n            correct_preds += (predicted_labels_batch == labels).sum().cpu()\n    \n    # Computing final accuracy score\n    test_accuracy_score = correct_preds.float() / num_examples\n\n    # Transforming a list of tensors into one tensor\n    test_predictions_tensor = torch.cat(test_predictions).cpu()\n            \n    return test_accuracy_score, test_predictions_tensor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Computing test accuracy and test predictions\naccuracy_test, predictions_test = evaluate_test(\n    model=bert_model, dataloader=testing_dataloader\n)\n\nprint(f\"Test accuracy: {accuracy_test:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing the fine-tuned model on new data","metadata":{}},{"cell_type":"code","source":"# Defining the Transformers pipeline\nbert_pipeline = pipeline(\n    task=\"text-classification\",\n    model=bert_model,\n    tokenizer=bert_tokenizer,\n    device=DEVICE,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predicting one text","metadata":{}},{"cell_type":"code","source":"def define_lang_one_text(pipeline, test_text):\n    \"\"\"Outputs the language prediction for a sample text.\"\"\"\n    # Applying the pipeline to make predictions\n    one_text_results = pipeline(test_text)[0]\n    # Retrieving the probability\n    proba = one_text_results[\"score\"]\n    # Retrieving the predicted label (encoded)\n    predicted_lang = one_text_results[\"label\"]\n    # Displaying the prediction\n    print(f\"Predicted language: {predicted_lang} ({proba:.2%} probability)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a test text (English)\ntest_text = \"It is interesting how we all turned out to be in the same place.\"\nprint(f\"Input text: {test_text}\\n\")\n\ndefine_lang_one_text(pipeline=bert_pipeline, test_text=test_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a test text (Dutch)\ntest_text = \"Ik kan het zien. Dankuwel!\"\nprint(f\"Input text: {test_text}\\n\")\n\ndefine_lang_one_text(pipeline=bert_pipeline, test_text=test_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a test text (Spanish)\ntest_text = \"Si yo fuera Maradona viviría como él\"\nprint(f\"Input text: {test_text}\\n\")\n\ndefine_lang_one_text(pipeline=bert_pipeline, test_text=test_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a test text (German)\ntest_text = \"Ich habe keine zeit. Auf Wiedersehen!\"\nprint(f\"Input text: {test_text}\\n\")\n\ndefine_lang_one_text(pipeline=bert_pipeline, test_text=test_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a test text (Russian)\ntest_text = \"Что бы я делал без тебя, я не знаю\"\nprint(f\"Input text: {test_text}\\n\")\n\ndefine_lang_one_text(pipeline=bert_pipeline, test_text=test_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a test text (Swedish)\ntest_text = \"Ursäkta mig! Hej då!\"\nprint(f\"Input text: {test_text}\\n\")\n\ndefine_lang_one_text(pipeline=bert_pipeline, test_text=test_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a test text (Portuguese)\ntest_text = \"Como chego à estação de trem? Como chego ao ponto de ônibus?\"\nprint(f\"Input text: {test_text}\\n\")\n\ndefine_lang_one_text(pipeline=bert_pipeline, test_text=test_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Predicting an array of texts","metadata":{}},{"cell_type":"code","source":"# Creating sample texts for classification\ntest_texts = [\n    \"It is impossible to do without a dictionary.\",\n    \"Из любой ситуации всегда есть выход. Нужно лишь его увидеть!\",\n    \"Eu gostaria de reservar um quarto, por favor\",\n    \"Hooplijk schrift zij niets negatiefs in haar recensie...\",\n    \"Hvor er der en pengeautomat?\",\n    \"Καλά, ευχαριστώ, Χάρηκα\",\n    \"Tünaydın!\",\n    \"Weisst was ich meine?\",\n    \"Interesting! I will look it up!\",\n]\n\n# Applying the pipeline on all test texts\nmultiple_texts_results = bert_pipeline(test_texts)\n\n# Outputting the results as DataFrame\nmultiple_texts_df = pd.DataFrame(multiple_texts_results)\nmultiple_texts_df[\"test_text\"] = test_texts\nmultiple_texts_df[\"predicted_lang\"] = multiple_texts_df[\"label\"].copy()\n\n# Rearranging the columns\nmultiple_texts_df[[\"test_text\", \"predicted_lang\", \"score\"]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vizualizing the predictions","metadata":{}},{"cell_type":"code","source":"print(classification_report(\n    y_true=testing_labels, \n    y_pred=predictions_test, \n    target_names=class_names,\n)\n     )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(\n    testing_labels, \n    predictions_test,\n    display_labels=class_names,\n    cmap=\"Blues\",\n)\nplt.xticks(rotation=90)\nplt.xlabel(\"Predicted language\")\nplt.ylabel(\"True language\")\nplt.title(\"Language recognition confusion matrix\", fontsize=15)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}